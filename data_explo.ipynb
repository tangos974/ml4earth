{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchgeo==0.6.0 torch==2.3.1 torchaudio==2.3.1 torchvision==0.18.1 opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from torchgeo.datasets import LandCoverAI\n",
    "\n",
    "# Define os agnositc root\n",
    "dataroot = pathlib.Path(\"data/landcoverai\")\n",
    "\n",
    "# This doesn't apply any transforms\n",
    "train_dataset = LandCoverAI(root=dataroot)\n",
    "val_dataset = LandCoverAI(root=dataroot, split=\"val\")\n",
    "test_dataset = LandCoverAI(root=dataroot, split=\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from torchgeo.datasets import LandCoverAI\n",
    "\n",
    "# Define os agnositc root\n",
    "dataroot = pathlib.Path(\"data/landcoverai\")\n",
    "\n",
    "# This doesn't apply any transforms\n",
    "train_dataset = LandCoverAI(root=dataroot)\n",
    "val_dataset = LandCoverAI(root=dataroot, split=\"val\")\n",
    "test_dataset = LandCoverAI(root=dataroot, split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sizes\n",
    "print(train_dataset.__len__())\n",
    "print(val_dataset.__len__())\n",
    "print(test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Let's visualize a few examples\n",
    "for i in range(3):\n",
    "    sample = train_dataset[random.randint(0, train_dataset.__len__()- 1)]\n",
    "    train_dataset.plot(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    sample = val_dataset[random.randint(0, val_dataset.__len__() - 1)]\n",
    "    train_dataset.plot(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import tensor\n",
    "\n",
    "def compute_mean_std(dataset: Dataset):\n",
    "    \"\"\"Compute the mean and standard deviation of a dataset.\"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=16, num_workers=0, shuffle=False)\n",
    "    \n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    n_samples = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        images = data['image']\n",
    "        batch_samples = images.size(0)  # Batch size (the number of images)\n",
    "        images = images.view(batch_samples, images.size(1), -1)  # Reshape to (B, C, H*W)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        n_samples += batch_samples\n",
    "\n",
    "        print(f\"Computed for {n_samples} samples out of {len(dataset)}\")\n",
    "    \n",
    "    mean /= n_samples\n",
    "    std /= n_samples\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# Use the training dataset to compute mean and std\n",
    "# mean_train, std_train = compute_mean_std(train_dataset)\n",
    "mean_train = tensor([ 94.2184, 101.2178,  87.5592])\n",
    "std_train = tensor([25.8982, 22.9285, 18.8788])\n",
    "\n",
    "# Use the validation dataset to compute mean and std\n",
    "# mean_val, std_val = compute_mean_std(val_dataset)\n",
    "mean_val = tensor([ 95.0307, 102.2367,  88.4823])\n",
    "std_val = tensor([26.2774, 23.1119, 19.1133])\n",
    "\n",
    "# Use the test dataset to compute mean and std\n",
    "# mean_test, std_test = compute_mean_std(test_dataset)\n",
    "mean_test = tensor([ 92.9187, 100.2754,  87.1114])\n",
    "std_test = tensor([25.5023, 22.6398, 18.5629])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall mean and std with weights corresponding to the number of samples\n",
    "\n",
    "# Compute the number of samples in each dataset\n",
    "n_train = len(train_dataset)\n",
    "n_val = len(val_dataset)\n",
    "n_test = len(test_dataset)\n",
    "\n",
    "# Compute the weights\n",
    "w_train = 1.0 / n_train\n",
    "w_val = 1.0 / n_val\n",
    "w_test = 1.0 / n_test\n",
    "\n",
    "# Compute the mean and std\n",
    "mean = (w_train * mean_train + w_val * mean_val + w_test * mean_test) / (w_train + w_val + w_test)\n",
    "std = (w_train * std_train + w_val * std_val + w_test * std_test) / (w_train + w_val + w_test)\n",
    "\n",
    "# Normalize to [0,1]\n",
    "MEAN = mean / 255.0\n",
    "STD = std / 255.0\n",
    "\n",
    "print(f\"Mean: {MEAN}\")\n",
    "print(f\"Std: {STD}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape mean and std to match the image dimensions\n",
    "mean = MEAN.view(3, 1, 1)\n",
    "std = STD.view(3, 1, 1)\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore shape of the dataset\n",
    "sample = train_dataset[0]\n",
    "image = sample[\"image\"]\n",
    "mask = sample[\"mask\"]\n",
    "print(f\"Original Image Shape: {image.shape}\")\n",
    "print(f\"Original Mask Shape: {mask.shape}\")\n",
    "\n",
    "# See scale of images\n",
    "print(f\"Max: {image.max()}\")\n",
    "print(f\"Min: {image.min()}\")\n",
    "\n",
    "# See scale of masks\n",
    "print(f\"Max: {mask.max()}\")\n",
    "print(f\"Min: {mask.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized image and mask\n",
    "for i in range(2):\n",
    "    sample = train_dataset[random.randint(0, train_dataset.__len__() - 1)]\n",
    "    print(f\"Original Image Shape: {sample['image'].shape} - Max: {sample['image'].max()} - Min: {sample['image'].min()}\")\n",
    "\n",
    "    # Since mean and std are in [0, 1], and image is in [0, 255], we need to scale image to [0, 1]\n",
    "    scaled_image = sample[\"image\"] / 255.0\n",
    "    print(f\"Scaled Image Shape: {scaled_image.shape} - Max: {scaled_image.max()} - Min: {scaled_image.min()}\")\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_image = (scaled_image - mean) / std\n",
    "    \n",
    "    train_dataset.plot(sample)\n",
    "    train_dataset.plot({\n",
    "        \"image\": normalized_image,\n",
    "        \"mask\": sample[\"mask\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see how it helps makes differences 'stand out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Class names for visualization (assuming you have 5 classes)\n",
    "class_names = [\"Background\", \"Building\", \"Woodland\", \"Water\", \"Road\"]\n",
    "\n",
    "# Initialize a dictionary to count pixels per class\n",
    "class_counts = defaultdict(int)\n",
    "\n",
    "# Iterate through the entire dataset\n",
    "for i in range(len(train_dataset)):\n",
    "    sample = train_dataset[i]\n",
    "    mask = sample['mask']  # Assuming the mask is a key in the sample dictionary\n",
    "    \n",
    "    # Count pixels for each class in the mask\n",
    "    for class_label in range(len(class_names)):\n",
    "        class_counts[class_label] += torch.sum(mask == class_label).item()\n",
    "\n",
    "# Total number of pixels\n",
    "total_pixels = sum(class_counts.values())\n",
    "\n",
    "# Calculate the distribution as a percentage\n",
    "class_distribution = {class_names[k]: (v / total_pixels) * 100 for k, v in class_counts.items()}\n",
    "\n",
    "# Display the distribution\n",
    "for class_name, percentage in class_distribution.items():\n",
    "    print(f\"Class {class_name}: {percentage:.2f}%\")\n",
    "\n",
    "# Optional: Visualize the distribution as a bar chart\n",
    "plt.bar(class_distribution.keys(), class_distribution.values())\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Percentage of Pixels')\n",
    "plt.title('Class Distribution in LandCoverAI Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_road_images = 0\n",
    "no_building_images = 0\n",
    "no_water_images = 0\n",
    "\n",
    "# Loop through the dataset\n",
    "for i in range(len(train_dataset)):\n",
    "    sample = train_dataset[i]\n",
    "    mask = sample['mask']\n",
    "    \n",
    "    # Check if any road, building, or water pixels exist\n",
    "    if torch.sum(mask == 1) == 0:\n",
    "        no_building_images += 1\n",
    "    if torch.sum(mask == 2) == 0:\n",
    "        no_water_images += 1\n",
    "    if torch.sum(mask == 4) == 0:\n",
    "        no_road_images += 1\n",
    "\n",
    "# Calculate the percentage of images for each class\n",
    "total_images = len(train_dataset)\n",
    "no_road_percentage = (no_road_images / total_images) * 100\n",
    "no_building_percentage = (no_building_images / total_images) * 100\n",
    "no_water_percentage = (no_water_images / total_images) * 100\n",
    "\n",
    "# Display the results\n",
    "print(f\"Percentage of images with no roads: {no_road_percentage:.2f}%\")\n",
    "print(f\"Percentage of images with no buildings: {no_building_percentage:.2f}%\")\n",
    "print(f\"Percentage of images with no water: {no_water_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['No Roads', 'No Buildings', 'No Water']\n",
    "percentages = [no_building_percentage, no_road_percentage, no_water_percentage]\n",
    "\n",
    "# Plotting the percentages\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(categories, percentages, color=['salmon', 'lightgrey', 'lightblue'])\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Percentage of Images (%)')\n",
    "plt.title('Percentage of Images Without Roads, Buildings, or Water')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class frequencies as percentages\n",
    "class_frequencies = [57.93, 0.86, 33.13, 6.46, 1.62]\n",
    "\n",
    "# Convert percentages to proportions\n",
    "class_proportions = [freq / 100 for freq in class_frequencies]\n",
    "\n",
    "# Compute inverse frequencies\n",
    "inverse_freqs = [1.0 / prop for prop in class_proportions]\n",
    "\n",
    "# Normalize weights to have a mean of 1\n",
    "mean_weight = sum(inverse_freqs) / len(inverse_freqs)\n",
    "class_weights = [w / mean_weight for w in inverse_freqs]\n",
    "class_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
